Valeryia Zaitsava 
Data Analytics Capstone - Ryerson University Detecting Fake Reviews

#For my capstone analytics project, I'll be using a dataset of Yelp reviews to see if I can identify opinion spam candidates, 
also known as fake reviews. The data comes from from Kaggle: https://www.kaggle.com/yelp-dataset/yelp-dataset/data called
'yelp_review.csv', and is 3.53 GB. The size of the dataset presents a challenge, especially because I need to first hand-label 
training data of fake and real reviews.

#First I import libraries and prepare the environment:

In [ ]:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from pandas import Series, DataFrame
from pandas.tools.plotting import scatter_matrix
import matplotlib.pyplot as plt
from pylab import rcParams
import seaborn as sb
from re import sub, compile
from sklearn import metrics, model_selection
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import normalize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import StratifiedKFold
from sklearn import metrics, cross_validation
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import sys
import os

%matplotlib inline
rcParams['figure.figsize'] = 5, 4
sb.set_style('whitegrid')

Now load data from CSV file and check its structure
df = pd.read_csv("C:/Users/vzaitsava/Documents/yelp_review.csv")
df.columns =['review_id','user_id','business_id','stars','date','text','useful','funny','cool']
df.head()

Out[]:
review_id	user_id	business_id	stars	date	text	useful	funny	cool
0	vkVSCC7xljjrAI4UGfnKEQ	bv2nCi5Qv5vroFiqKGopiw	AEx2SYEUJmTxVVB18LlCwA	5	2016-05-28	Super simple place but amazing nonetheless. It...	0	0	0
1	n6QzIUObkYshz4dz2QRJTw	bv2nCi5Qv5vroFiqKGopiw	VR6GpWIda3SfvPC-lg9H3w	5	2016-05-28	Small unassuming place that changes their menu...	0	0	0
2	MV3CcKScW05u5LVfF6ok0g	bv2nCi5Qv5vroFiqKGopiw	CKC0-MOWMqoeWf6s-szl8g	5	2016-05-28	Lester's is located in a beautiful neighborhoo...	0	0	0
3	IXvOzsEMYtiJI0CARmj77Q	bv2nCi5Qv5vroFiqKGopiw	ACFtxLv8pGrrxMm6EgjreA	4	2016-05-28	Love coming here. Yes the place always needs t...	0	0	0
4	L_9BTb55X0GDtThi6GlZ6w	bv2nCi5Qv5vroFiqKGopiw	s2I_Ni76bjJNK9yG60iD-Q	4	2016-05-28	Had their chocolate almond croissant and it wa...	0	0	0

df['review_id'].count()
Out[]:
5261668

df['user_id'].nunique()
Out[]:
1326101

#So the dataset contains 5,261,668 reviews by 1,326,101 reviewers. There are no missing values.

#From literature review I found that reviewers who post more than 3-5 times per day are strong candidates for being opinion spammers.
Legitimate reviews rarely post more than 1-2 reviews per day. Therefore I need to group my data by user_id and calculate how many
times on average they post per day, organizing my new dataframe as follows: | user_id | avg_daily_review_rate |.
Then I can identify top frequent daily reviewers and look at their individual reviews more in-depth to see if I can hand-label them
as fake. After I can look at low daily posters and their reviews, for creating potential 'true' review labels.

#First, calculate reviewers' average daily review rate.

users_reviews = df.groupby((['user_id','date']), as_index=False).count()
adrr = users_reviews.groupby('user_id', as_index=False)['text'].mean()
adrr.head()
Out[76]:
	               user_id      text
0	---1lKK3aKOuomHnwAkAow	2.333333
1	---94vtJ_5o_nikEs6hUjg	1.000000
2	---PLwSf5gKdIoVnyRHgBA	1.000000
3	---cu1hq55BP9DWVXXKHZg	1.000000
4	---fhiwiwBYrvqhpXgcWDQ	1.000000

adrr['text'].sort_values(ascending=False)
Out[]:
767544     45.0
90034      44.0
1293977    38.0
1181066    37.0
433825     34.5
           ... 
823671      1.0
823670      1.0
823669      1.0
823668      1.0
663050      1.0
Name: text, Length: 1326101, dtype: float64

#Top daily poster has 45 reviews/day on average. Lowest are at 1/day. Let's take a look at the reviews of the top daily posting user.

adrr[adrr['text'] == 45]
Out []:
                       user_id	text
767544	_0TKEDIcuWW5Y4BHo1t24g	45.0

adrr[adrr['user_id'] == '_0TKEDIcuWW5Y4BHo1t24g']
Out[]:
review_id	                      user_id	                business_id	            stars	date	  text	                                        useful funny	cool
3440771	M6-isJkcNwKyRjWQ0HV5cA	_0TKEDIcuWW5Y4BHo1t24g	NcNzTKVatkbRkD_G8A891A	3	2014-09-13	It's pretty good but a little too pricey. The...	0	0	0
3440772	S_bAcDjztyHyYj4ectpXfw	_0TKEDIcuWW5Y4BHo1t24g	MtjOk7u7sp3yWyzscAIG4Q	5	2014-09-13	I really like the sandwiches and fries. The p...	0	0	0
3440773	iIZJykODmOjFvg0SnkyKlQ	_0TKEDIcuWW5Y4BHo1t24g	VxRlBe2wjtycFWSZm1orTA	5	2014-09-13	Love it. It's hard to resist the Fish and Chi...	0	0	0
3440774	5vvtC33vV5a_jUUV8OXaqg	_0TKEDIcuWW5Y4BHo1t24g	TpZO3sokGBg8mDYyTRiGiQ	4	2014-09-13	Good food but it's hard to eat there on a budg...	0	0	0
3440775	JJHRUEGjpSbo3DDvVQ0fFw	_0TKEDIcuWW5Y4BHo1t24g	04kZ5CSh6oKhI5huU5bLdg	4	2014-09-13	The beer list used to be unrivaled in Pittsbur...	0	0	0
3440776	w-GOvipPIc5S70W6ztmT-g	_0TKEDIcuWW5Y4BHo1t24g	7aZf5c1UNotq4MabBXMZLA	5	2014-09-13	Great overlooked Chinese place in Squill. It'...	0	0	0
3440777	S0Kni4Kqs5NzfJfwBib2Jw	_0TKEDIcuWW5Y4BHo1t24g	J9f-9Prw2YVM-fiZqv2fmQ	2	2014-09-13	Awful hot dogs, greasy ass carnie fries that a...	0	0	0
3440778	pCvAERNXNMm2WvxphuMAZA	_0TKEDIcuWW5Y4BHo1t24g	Zja7ykq3icDzkN20XX6j1w	5	2014-09-13	Way better than Mineos. A slicer of Pepperoni...	0	0	0
3440779	W_PHAJ3DEU50t4GnBpb1sQ	_0TKEDIcuWW5Y4BHo1t24g	iK_3WktnHa_YlbWdA0Axkw	5	2014-09-13	Loved it. Nice little secret in a crummy area...	0	0	0
3440780	tanWeoTD3dWXw0CMnvKIlQ	_0TKEDIcuWW5Y4BHo1t24g	RThuRlhMdowhtY8vITEIzQ	1	2014-09-13	Worst & most overrated Pizza in the 'Burgh. P...	1	0	1
3440781	5KSOnL3smj_fBCcEdiMxVg	_0TKEDIcuWW5Y4BHo1t24g	S8VRQCBrbZI04zjCOc1RAA	5	2014-09-13	Two dollar authentic tacos seven days a week w...	0	0	0
3440782	-gybE83bVC51sUmmJBMsOA	_0TKEDIcuWW5Y4BHo1t24g	gaDAgdozCoSRN23WU4PolA	5	2014-09-13	It's hard to find out where they're going to b...	0	0	0
3440783	zJ-lf2UGv-5X99bLabfH2A	_0TKEDIcuWW5Y4BHo1t24g	BIqsItY0vyOshMpJ0I4cPg	5	2014-09-13	Great authentic menu & their American Chinese ...	0	0	0
3440784	vB9DVLh5yKvrmMxi3jwNFw	_0TKEDIcuWW5Y4BHo1t24g	7wMB3wMbOp0gq3l6nHfOaA	4	2014-09-13	Pretty good ramen place. The menu doesn't dis...	0	0	0
3440785	q3ehIzmG5g7C5D5QBxkjMQ	_0TKEDIcuWW5Y4BHo1t24g	sMzNLdhJZGzYirIWt-fMAg	5	2014-09-13	I like to get a salad with some meatbals and f...	0	0	0
3440786	nQeFNBVCMmLaF-kxLMZ3sQ	_0TKEDIcuWW5Y4BHo1t24g	XItYW5ul3OW_AqpT2nDbBQ	5	2014-09-13	If you love point brugge, you're going to love...	0	0	0
3440787	uSvnktoGThhXXodow5zBYw	_0TKEDIcuWW5Y4BHo1t24g	CK-Gv3vqIlWOrKP4fhT8_g	4	2014-09-13	It's pretty good bar food, but usually you end...	0	0	0
3440788	VF1lzbJre3Feqv6OLzzX8A	_0TKEDIcuWW5Y4BHo1t24g	X2X3n0PutSNonQQj2cjsCw	5	2014-09-13	It's nice going to a Vietnamese place that's m...	0	0	0
3440789	1WVMF-nJEPpzEQIyfdtepg	_0TKEDIcuWW5Y4BHo1t24g	Fpm3WvqtrAg2ueh_4pz7iA	5	2014-09-13	Best Thai in Pittsburgh. Love it. Every nood...	0	0	0
3440790	jFn7vDlzYNMmIrHcOi2jfQ	_0TKEDIcuWW5Y4BHo1t24g	qHseX2NHeUUedIgs_VasZA	5	2014-09-13	Stinks that it's on the South Side, but La Pal...	0	0	0
3440791	vECPsIAevZ1CWsxgUGdJJA	_0TKEDIcuWW5Y4BHo1t24g	e2zO5THDYoDOcyXcWnQv5w	4	2014-09-13	I've only ever gone late at night for the burg...	0	0	0
3440792	h645UMBPGzZIhS9hvOsr5A	_0TKEDIcuWW5Y4BHo1t24g	AKQbrvRBZvU5kB9Ut4gVkg	5	2014-09-13	Pretty awesome woodfire pizza. Dig it.	0	0	0
3440793	K53MwLBFFq0uvaQpa4M0fQ	_0TKEDIcuWW5Y4BHo1t24g	QqbZA4r5g1vCbOxOAKJgWg	5	2014-09-13	Pretty righteous burgers, but they are their o...	0	0	0
3440794	pdqwqfeIp3IbNx7lcCIejQ	_0TKEDIcuWW5Y4BHo1t24g	u4sTiCzVeIHZY8OlaL346Q	5	2014-09-13	Gigantic amazing sandwiches. Yeah. Freaking ...	0	0	0
3440795	C7-woP2CtpuVGhfUQaS03Q	_0TKEDIcuWW5Y4BHo1t24g	GPoF-Bubt3r289NpTGAcsw	4	2014-09-13	Better than BRGR, but I thought the mediterra ...	0	0	0
3440796	7Bw2rM84Z6OSgETXDbNv6g	_0TKEDIcuWW5Y4BHo1t24g	ejaUQ1hYo7Q7xCL1HdPINw	3	2014-09-13	They don't brew a single good beer, menu's ver...	0	0	0
3440797	mBZE55deqdJbE1W9-1lC5g	_0TKEDIcuWW5Y4BHo1t24g	Bg8xZT4-NsYLYd-WPIhgXw	5	2014-09-13	Great hot dogs and fries and tacos. I love th...	0	0	0
3440798	n47tbhgxYy0sgk0ZxF7aoQ	_0TKEDIcuWW5Y4BHo1t24g	hcFSc0OHgZJybnjQBrL_8Q	5	2014-09-13	Terrific food, I've eaten there at least a doz...	0	0	0
3440799	1rfTmOH1pmdnNKt0T9jQZw	_0TKEDIcuWW5Y4BHo1t24g	hUCujBvgttk5jNh_uQwi2Q	4	2014-09-13	It's pretty righteous. I prefer Sola, but I d...	0	0	0
3440800	cNnRa2lqafBqmznSNdoAPw	_0TKEDIcuWW5Y4BHo1t24g	q-BIBN88JkOXaK2FIlPpuQ	5	2014-09-13	Righteos tacos. Chorizo's my favorite. Thai ...	0	0	0
3440801	tG7es3zomPAcapzEruvzqg	_0TKEDIcuWW5Y4BHo1t24g	Oy9xDxD-JtUzSurhOhboLg	5	2014-09-13	Awesome freaking waffles. Nutella, speculoos ...	0	0	0
3440802	Jo-Zu5ZiGrAa9LVMjybPhQ	_0TKEDIcuWW5Y4BHo1t24g	2v3uI97JPebCDaTNy8jdwg	5	2014-09-13	Best NY-style slicery in the Burgh. Love thei...	0	0	0
3440803	wJEQ8MR8FeFlT1Raei7tUg	_0TKEDIcuWW5Y4BHo1t24g	RvwZqjdkZ_pER0moPXLZAQ	4	2014-09-13	Terrific hotpots. The short rib is really rea...	0	0	0
3440804	OBP9cDFoPmC_i8WCurtnUQ	_0TKEDIcuWW5Y4BHo1t24g	A4kCvILTgfr_2cG0_yGNiw	5	2014-09-13	Really enjoyed the atmosphere, the Bibimbop, t...	0	0	0
3440805	q84nwqxuOBJgqFM5aaWSHw	_0TKEDIcuWW5Y4BHo1t24g	AspFf1d9sRaUloPJAHMqAg	5	2014-09-13	Notice the word Kim in the name? That's Korea...	0	0	0
3440806	k7d-iWOdItuLFEsgbD7Lwg	_0TKEDIcuWW5Y4BHo1t24g	X4YG1pu1MDVSi07S3i4YCA	5	2014-09-13	This is what Chinese takeout should be. This ...	0	0	0
3440807	tvzcDRCW0XaFFXhJQ4Coeg	_0TKEDIcuWW5Y4BHo1t24g	lKom12WnYEjH5FFemK3M1Q	3	2014-09-13	It's fun to have one of their disgusting sandw...	0	0	0
3440808	_EXnlz_2pUFb4sYvPTH3BQ	_0TKEDIcuWW5Y4BHo1t24g	z15C-dJodOlPO8bMBl6F8g	5	2014-09-13	Started going there for No Menu Monday. It's ...	0	0	0
3440809	ulhjp0532vfwXMkcxbxl3A	_0TKEDIcuWW5Y4BHo1t24g	qQvHYQ1uocuEnFcUdhxV_g	5	2014-09-13	The whole menu is pretty righteous. The Rippe...	0	0	0
3440810	Tkkg2WIOKNX0XOMK3xRfrQ	_0TKEDIcuWW5Y4BHo1t24g	Gm2JaFiMaNDP3RsYociz9w	3	2014-09-13	I thought the fries were pretty good, but I wa...	0	0	0
3440811	9GgKwNzS1YlVks299exFlA	_0TKEDIcuWW5Y4BHo1t24g	2f90tMt0SFkwBJ6exobY3Q	5	2014-09-13	A lot of people go here for the booze, atmosph...	0	0	0
3440812	KWhONBEBSPFOqDWlD7Eh6g	_0TKEDIcuWW5Y4BHo1t24g	szlIcLQE7K-1JuKeJIRKsQ	4	2014-09-13	The service is always super friendly there and...	0	0	0
3440813	0YzcFa4k9flQm-OyXhUsqA	_0TKEDIcuWW5Y4BHo1t24g	2UCXCbv7qaO3d8FfQd4Ing	3	2014-09-13	This is what we order when we deliver. If Aie...	0	0	0
3440814	ZQs4zTVFJL8pAtbcdrSf9g	_0TKEDIcuWW5Y4BHo1t24g	0PCBt3JKD6IooicImKNBzA	4	2014-09-13	It was good, but expensive and the portions we...	0	0	0
3440815	K8fS7NS7JLdg880Z8U7Jzw	_0TKEDIcuWW5Y4BHo1t24g	3GuqcEYaF1d1WXcwOX5xPw	4	2014-09-13	A little pricey, but the food's really good. ...	0	0	0

#We can see that this user wrote more than 20 reviews on 2014-09-13 and most of their reviews were positive, 5 or 4 stars,
and the reviews sounds generic and like they can be interchanged no matter what business is being reviewed. 
This makes the user a great candidate for a spammer and I would label their reviews as 'fake'. 

#This indicates that it's viable approach to flag fake reviewers based on how frequently they post reviews.
The reviews of the users who post 2 reviews per day or I will label as 'fake' and the ones with less than 2 reviews on average as 'real'.

real = adrr[adrr.text <= 2][:500]
fake = adrr[adrr.text > 2][:500]

realReviewsByUser = {}
for x in real.user_id:
    realReviewsByUser[x] = df[df.user_id == x].text.tolist()

fakeReviewsByUser = {}
for x in fake.user_id:
    fakeReviewsByUser[x] = df[df.user_id == x].text.tolist()

import itertools
allReals = itertools.chain(list(realReviewsByUser.values()))
allReals = list(allReals)
allFakes = itertools.chain(list(fakeReviewsByUser.values()))
allFakes = list(allFakes)

allReals = [y for x in allReals for y in x]
allFakes = [y for x in allFakes for y in x]

#Now that I generated labeled reviews, let's do comparison wordclouds to see if words used are signifiantly different.

wordcloud = WordCloud(
                          stopwords=STOPWORDS,
                          background_color='white',
                          width=1200,
                          height=1000
                         ).generate(" ".join(allFakes))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

wordcloud = WordCloud(
                          stopwords=STOPWORDS,
                          background_color='white',
                          width=1200,
                          height=1000
                         ).generate(" ".join(allReals))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

#The wordclouds are quite similar, in that they both use the words 'good', 'place', 'one', and 'food' the most. 'Time' and 'food' are 
more common among real reviewers, whereas 'really', 'nice', 'great' are more frequntly used in the fake reviews. This is along the lines
of previous research that shows that real users tend to be more concrete, whereas fake reviews tend to use flowery adjectives that can
make any place seem appealing.

#Now I can use the dataset of labelled reviews for training/testing/valudation. I will test 4 simple supervised learning models:
Linear SVM, SVM, decision tree, and Multinomial Naive Bayes.

realLabel = []
for x in range(0, len(allReals)):
    realLabel.append(1)
fakeLabel = []
for x in range(0, len(allFakes)):
    fakeLabel.append(0)

realDF = pd.DataFrame({'Text': allReals, 'Label': realLabel})
fakeDF = pd.DataFrame({'Text': allFakes, 'Label': fakeLabel})

dfs = [realDF, fakeDF]
df_final = pd.concat(dfs)
df_final = df_final.sample(frac=1).reset_index(drop=True)

labels = df_final.pop('Label')
features_list = df_final.pop('Text')
vectorizer = TfidfVectorizer(ngram_range=(1, 2))
X_ngrams = vectorizer.fit_transform(features_list)

X_train, X_test, y_train, y_test = train_test_split(
    X_ngrams,
    labels,
    test_size=0.3,
    random_state=42,
    stratify=labels
)

#First model I look at is LinearSVM.
clf = LinearSVC(loss='hinge')
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
linearSVC_score = metrics.accuracy_score(y_test, y_pred) * 100
print(linearSVC_score)

Out []: 
80.0837111964

#Now consider the Multinomial Naive Bayes.
clf = MultinomialNB()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
MNB_score = metrics.accuracy_score(y_test, y_pred) * 100
print(MNB_score)

Out []:
77.9909312871

#Compare to SVM model.
clf = SVC()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
SVC_score = metrics.accuracy_score(y_test, y_pred) * 100
print(SVC_score)

Out []: 
77.8862922916

#Last model considered is the Decison Tree classifier.
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
DCT_score = metrics.accuracy_score(y_test, y_pred) * 100
print(DCT_score)

Out []:
71.3289152424

#Let's compare the results visually side by side by plotting the model results.
algo_list = ['Linear SVM', 'Naive Bayes', 'SVM', 'Decision Tree']
values_list = [linearSVC_score, MNB_score, SVC_score, DCT_score]
plt.bar(algo_list, values_list)
plt.title = 'Algo Comparison'
plt.show()

#The Linear SVM model produced the best results.

#Now let's consider the length of reviews.
import matplotlib.pyplot as plt
uni_list = adrr.text.unique()
print(len(uni_list))
dic_graph = {}
for z in uni_list:
        if str(round(z)) in dic_graph: 
            dic_graph[str(round(z))] += (len(adrr[adrr.text == z]))
        else:
            dic_graph[str(round(z))] = (len(adrr[adrr.text == z]))
print(dic_graph)
dic_graph['4.0'] = 0
print(dic_graph)

Out []:
3980
{'9.0': 274, '32.0': 1, '1.0': 1114870, '22.0': 2, '3.0': 26030, '2.0': 166257, '18.0': 8, '37.0': 1, '45.0': 1, '10.0': 205, 
'12.0': 77, '17.0': 17, '38.0': 1, '25.0': 1, '7.0': 883, '16.0': 24, '14.0': 34, '28.0': 3, '24.0': 6, '44.0': 1, '8.0': 588, 
'5.0': 3563, '15.0': 18, '21.0': 6, '11.0': 132, '34.0': 3, '31.0': 1, '13.0': 44, '26.0': 4, '6.0': 2031, '19.0': 4, '4.0': 11005, 
'20.0': 6}
{'9.0': 274, '32.0': 1, '1.0': 1114870, '22.0': 2, '3.0': 26030, '2.0': 166257, '18.0': 8, '37.0': 1, '45.0': 1, '10.0': 205, 
'12.0': 77, '17.0': 17, '38.0': 1, '25.0': 1, '7.0': 883, '16.0': 24, '14.0': 34, '28.0': 3, '24.0': 6, '44.0': 1, '8.0': 588, 
'5.0': 3563, '15.0': 18, '21.0': 6, '11.0': 132, '34.0': 3, '31.0': 1, '13.0': 44, '26.0': 4, '6.0': 2031, '19.0': 4, '4.0': 0,
'20.0': 6}

import itertools
D2=dict(itertools.islice(dic_graph.items(),10))
print(D2)

Out[]:
{'9.0': 274, '32.0': 1, '1.0': 1114870, '22.0': 2, '18.0': 8, '3.0': 26030, '10.0': 205, '45.0': 1, '37.0': 1, '2.0': 166257}

#Each category or key in the dictionary is the average daily review posting number. We can see that most users post 1 review per day
on average - there are 1,114,870 of them. This is consistent with what we know about spam reviews - most reviews will tend to be real
(as indicated by infrequent posters). Let's look at this graphically.

import matplotlib.pyplot as plt
names = list(D2.keys())
values = list(D2.values())
plt.bar(range(len(D2)),values,tick_label=names)
plt.show()

#Now let's look at how we can characterie real user and fake users. 
import spacy
nlp = spacy.load('en')
real_users_df = avg_daily_review_rate[avg_daily_review_rate.text <= 3]
real_users_df.count()
real_users_df_truncate = real_users_df[:-1300000] 
real_users_df_truncate.count()

Out []:
user_id    5174
text       5174
dtype: int64

fake_users_df = avg_daily_review_rate[avg_daily_review_rate.text > 2]
fake_users_df.count()

Out[]:
user_id    61380
text       61380
dtype: int64

temptest = 0
avg_res=0
total = 0
Real_dic = {}
similarity_dic = {}
for x in avg_daily_review_rate['user_id']:
    if temptest <= 10:
        temptest+=1
        count = 0     
        select_user_reviews = df[df['user_id'] == x]
        Real_dic[x] = {}
        total = len(select_user_reviews)
        review = (select_user_reviews['text'].iloc[0])   
        for y in range(1,len(select_user_reviews)-1):        
            labrador = nlp(review)           
            in_review = nlp(select_user_reviews['text'].iloc[y])
            count+=float((labrador.similarity(in_review)))        
        avg_res = count/total
        if str(round(avg_daily_review_rate[avg_daily_review_rate['user_id'] == x]['text'],1)) in similarity_dic:
            similarity_dic[str(round(float(avg_daily_review_rate[avg_daily_review_rate['user_id'] == x]['text']),1))] += avg_res
        else:
            similarity_dic[str(round(float(avg_daily_review_rate[avg_daily_review_rate['user_id'] == x]['text']),1))] = avg_res
    else:
        break
print(similarity_dic)
